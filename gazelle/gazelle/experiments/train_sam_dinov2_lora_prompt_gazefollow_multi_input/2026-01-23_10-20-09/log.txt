2026-01-23 10:20:09,142 - Experiment Config: {'model': 'sam_sam_vitb', 'data_path': '/mnt/nvme1n1/lululemon/xjj/datasets/resized/gazefollow_extended', 'ckpt_save_dir': './experiments', 'wandb_project': 'sam_dinov2_vitb_lora_multi_input', 'exp_name': 'train_sam_dinov2_lora_prompt_gazefollow_multi_input', 'log_iter': 10, 'max_epochs': 15, 'batch_size': 60, 'lr': 0.001, 'n_workers': 8}
2026-01-23 10:20:12,036 - Learnable parameters: 3990496
2026-01-23 10:20:18,488 - 
[Epoch 0 Training]
2026-01-23 10:20:20,655 - Iter 0/1891, Loss=1.2754, Loss=1.2754
2026-01-23 10:20:24,480 - Iter 10/1891, Loss=0.0741, Loss=0.0741
2026-01-23 10:20:28,287 - Iter 20/1891, Loss=0.0699, Loss=0.0699
2026-01-23 10:20:32,068 - Iter 30/1891, Loss=0.0699, Loss=0.0699
2026-01-23 10:20:35,884 - Iter 40/1891, Loss=0.0692, Loss=0.0692
2026-01-23 10:20:39,675 - Iter 50/1891, Loss=0.0705, Loss=0.0705
2026-01-23 10:20:43,549 - Iter 60/1891, Loss=0.0706, Loss=0.0706
2026-01-23 10:20:47,361 - Iter 70/1891, Loss=0.0713, Loss=0.0713
2026-01-23 10:20:51,145 - Iter 80/1891, Loss=0.0688, Loss=0.0688
2026-01-23 10:20:54,971 - Iter 90/1891, Loss=0.0704, Loss=0.0704
2026-01-23 10:20:58,705 - Iter 100/1891, Loss=0.0669, Loss=0.0669
2026-01-23 10:21:02,526 - Iter 110/1891, Loss=0.0700, Loss=0.0700
2026-01-23 10:21:06,287 - Iter 120/1891, Loss=0.0679, Loss=0.0679
2026-01-23 10:21:10,062 - Iter 130/1891, Loss=0.0701, Loss=0.0701
2026-01-23 10:21:13,854 - Iter 140/1891, Loss=0.0699, Loss=0.0699
2026-01-23 10:21:17,636 - Iter 150/1891, Loss=0.0678, Loss=0.0678
2026-01-23 10:21:21,412 - Iter 160/1891, Loss=0.0705, Loss=0.0705
2026-01-23 10:21:25,208 - Iter 170/1891, Loss=0.0687, Loss=0.0687
2026-01-23 10:21:28,984 - Iter 180/1891, Loss=0.0689, Loss=0.0689
2026-01-23 10:21:32,781 - Iter 190/1891, Loss=0.0700, Loss=0.0700
2026-01-23 10:21:36,658 - Iter 200/1891, Loss=0.0687, Loss=0.0687
2026-01-23 10:21:40,521 - Iter 210/1891, Loss=0.0678, Loss=0.0678
2026-01-23 10:21:44,346 - Iter 220/1891, Loss=0.0685, Loss=0.0685
2026-01-23 10:21:48,144 - Iter 230/1891, Loss=0.0697, Loss=0.0697
2026-01-23 10:21:51,978 - Iter 240/1891, Loss=0.0700, Loss=0.0700
2026-01-23 10:21:55,784 - Iter 250/1891, Loss=0.0692, Loss=0.0692
2026-01-23 10:21:59,596 - Iter 260/1891, Loss=0.0676, Loss=0.0676
2026-01-23 10:22:03,395 - Iter 270/1891, Loss=0.0686, Loss=0.0686
2026-01-23 10:22:07,163 - Iter 280/1891, Loss=0.0677, Loss=0.0677
2026-01-23 10:22:10,954 - Iter 290/1891, Loss=0.0699, Loss=0.0699
2026-01-23 10:22:14,743 - Iter 300/1891, Loss=0.0690, Loss=0.0690
2026-01-23 10:22:18,486 - Iter 310/1891, Loss=0.0683, Loss=0.0683
2026-01-23 10:22:22,272 - Iter 320/1891, Loss=0.0696, Loss=0.0696
2026-01-23 10:22:26,056 - Iter 330/1891, Loss=0.0695, Loss=0.0695
2026-01-23 10:22:29,888 - Iter 340/1891, Loss=0.0694, Loss=0.0694
