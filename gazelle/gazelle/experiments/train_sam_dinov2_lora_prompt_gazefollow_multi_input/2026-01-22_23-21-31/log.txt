2026-01-22 23:21:31,806 - Experiment Config: {'model': 'sam_dinov2_vitb_lora_multi_input', 'data_path': '/mnt/nvme1n1/lululemon/xjj/datasets/resized/gazefollow_extended', 'ckpt_save_dir': './experiments', 'wandb_project': 'sam_dinov2_vitb_lora_multi_input', 'exp_name': 'train_sam_dinov2_lora_prompt_gazefollow_multi_input', 'log_iter': 10, 'max_epochs': 15, 'batch_size': 60, 'lr': 0.001, 'n_workers': 8}
2026-01-22 23:23:46,669 - Learnable parameters: 5432416
2026-01-22 23:23:52,649 - 
[Epoch 0 Training]
2026-01-22 23:23:55,626 - Iter 0/1891, Loss=9.8542, Loss=9.8542
2026-01-22 23:24:04,372 - Iter 10/1891, Loss=0.0833, Loss=0.0833
2026-01-22 23:24:12,910 - Iter 20/1891, Loss=0.0743, Loss=0.0743
2026-01-22 23:24:21,435 - Iter 30/1891, Loss=0.0738, Loss=0.0738
2026-01-22 23:24:29,942 - Iter 40/1891, Loss=0.0720, Loss=0.0720
2026-01-22 23:24:38,452 - Iter 50/1891, Loss=0.0706, Loss=0.0706
2026-01-22 23:24:46,998 - Iter 60/1891, Loss=0.0716, Loss=0.0716
2026-01-22 23:24:55,535 - Iter 70/1891, Loss=0.0698, Loss=0.0698
2026-01-22 23:25:04,063 - Iter 80/1891, Loss=0.0698, Loss=0.0698
2026-01-22 23:25:12,603 - Iter 90/1891, Loss=0.0705, Loss=0.0705
2026-01-22 23:25:21,091 - Iter 100/1891, Loss=0.0695, Loss=0.0695
2026-01-22 23:25:29,545 - Iter 110/1891, Loss=0.0698, Loss=0.0698
2026-01-22 23:25:37,981 - Iter 120/1891, Loss=0.0701, Loss=0.0701
2026-01-22 23:25:46,564 - Iter 130/1891, Loss=0.0691, Loss=0.0691
2026-01-22 23:25:55,090 - Iter 140/1891, Loss=0.0704, Loss=0.0704
2026-01-22 23:26:03,553 - Iter 150/1891, Loss=0.0690, Loss=0.0690
2026-01-22 23:26:12,045 - Iter 160/1891, Loss=0.0681, Loss=0.0681
2026-01-22 23:26:20,579 - Iter 170/1891, Loss=0.0704, Loss=0.0704
2026-01-22 23:26:29,029 - Iter 180/1891, Loss=0.0700, Loss=0.0700
2026-01-22 23:26:37,575 - Iter 190/1891, Loss=0.0693, Loss=0.0693
2026-01-22 23:26:46,101 - Iter 200/1891, Loss=0.0697, Loss=0.0697
2026-01-22 23:26:54,642 - Iter 210/1891, Loss=0.0685, Loss=0.0685
2026-01-22 23:27:03,138 - Iter 220/1891, Loss=0.0696, Loss=0.0696
2026-01-22 23:27:11,668 - Iter 230/1891, Loss=0.0679, Loss=0.0679
2026-01-22 23:27:20,202 - Iter 240/1891, Loss=0.0680, Loss=0.0680
2026-01-22 23:27:28,736 - Iter 250/1891, Loss=0.0683, Loss=0.0683
2026-01-22 23:27:37,254 - Iter 260/1891, Loss=0.0683, Loss=0.0683
2026-01-22 23:27:45,801 - Iter 270/1891, Loss=0.0687, Loss=0.0687
2026-01-22 23:27:54,305 - Iter 280/1891, Loss=0.0681, Loss=0.0681
2026-01-22 23:28:02,844 - Iter 290/1891, Loss=0.0676, Loss=0.0676
2026-01-22 23:28:11,278 - Iter 300/1891, Loss=0.0682, Loss=0.0682
2026-01-22 23:28:19,815 - Iter 310/1891, Loss=0.0679, Loss=0.0679
2026-01-22 23:28:28,313 - Iter 320/1891, Loss=0.0696, Loss=0.0696
2026-01-22 23:28:36,824 - Iter 330/1891, Loss=0.0691, Loss=0.0691
2026-01-22 23:28:45,281 - Iter 340/1891, Loss=0.0686, Loss=0.0686
2026-01-22 23:28:53,772 - Iter 350/1891, Loss=0.0699, Loss=0.0699
2026-01-22 23:29:02,251 - Iter 360/1891, Loss=0.0691, Loss=0.0691
2026-01-22 23:29:10,751 - Iter 370/1891, Loss=0.0698, Loss=0.0698
2026-01-22 23:29:19,217 - Iter 380/1891, Loss=0.0689, Loss=0.0689
2026-01-22 23:29:27,704 - Iter 390/1891, Loss=0.0686, Loss=0.0686
