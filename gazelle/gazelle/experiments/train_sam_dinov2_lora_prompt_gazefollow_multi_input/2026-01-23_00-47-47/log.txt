2026-01-23 00:47:47,811 - Experiment Config: {'model': 'sam_sam_vitb_lora', 'data_path': '/mnt/nvme1n1/lululemon/xjj/datasets/resized/gazefollow_extended', 'ckpt_save_dir': './experiments', 'wandb_project': 'sam_dinov2_vitb_lora_multi_input', 'exp_name': 'train_sam_dinov2_lora_prompt_gazefollow_multi_input', 'log_iter': 10, 'max_epochs': 15, 'batch_size': 60, 'lr': 0.001, 'n_workers': 8}
2026-01-23 00:47:50,676 - Learnable parameters: 4285408
2026-01-23 00:47:56,335 - 
[Epoch 0 Training]
2026-01-23 00:47:58,759 - Iter 0/1891, Loss=1.4866, Loss=1.4866
2026-01-23 00:48:05,151 - Iter 10/1891, Loss=0.0760, Loss=0.0760
2026-01-23 00:48:11,604 - Iter 20/1891, Loss=0.0712, Loss=0.0712
2026-01-23 00:48:18,110 - Iter 30/1891, Loss=0.0719, Loss=0.0719
2026-01-23 00:48:24,596 - Iter 40/1891, Loss=0.0707, Loss=0.0707
2026-01-23 00:48:31,039 - Iter 50/1891, Loss=0.0703, Loss=0.0703
2026-01-23 00:48:37,574 - Iter 60/1891, Loss=0.0686, Loss=0.0686
2026-01-23 00:48:44,117 - Iter 70/1891, Loss=0.0700, Loss=0.0700
2026-01-23 00:48:50,647 - Iter 80/1891, Loss=0.0701, Loss=0.0701
2026-01-23 00:48:57,202 - Iter 90/1891, Loss=0.0680, Loss=0.0680
2026-01-23 00:49:03,751 - Iter 100/1891, Loss=0.0702, Loss=0.0702
2026-01-23 00:49:10,255 - Iter 110/1891, Loss=0.0672, Loss=0.0672
2026-01-23 00:49:16,773 - Iter 120/1891, Loss=0.0683, Loss=0.0683
2026-01-23 00:49:23,328 - Iter 130/1891, Loss=0.0692, Loss=0.0692
2026-01-23 00:49:29,872 - Iter 140/1891, Loss=0.0672, Loss=0.0672
2026-01-23 00:49:36,382 - Iter 150/1891, Loss=0.0677, Loss=0.0677
2026-01-23 00:49:42,891 - Iter 160/1891, Loss=0.0690, Loss=0.0690
2026-01-23 00:49:49,435 - Iter 170/1891, Loss=0.0683, Loss=0.0683
2026-01-23 00:49:55,944 - Iter 180/1891, Loss=0.0680, Loss=0.0680
2026-01-23 00:50:02,465 - Iter 190/1891, Loss=0.0697, Loss=0.0697
