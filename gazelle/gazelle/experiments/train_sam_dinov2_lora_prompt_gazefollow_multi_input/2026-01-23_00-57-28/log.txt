2026-01-23 00:57:28,892 - Experiment Config: {'model': 'sam_dinov2_vitb_lora', 'data_path': '/mnt/nvme1n1/lululemon/xjj/datasets/resized/gazefollow_extended', 'ckpt_save_dir': './experiments', 'wandb_project': 'sam_dinov2_vitb_lora_multi_input', 'exp_name': 'train_sam_dinov2_lora_prompt_gazefollow_multi_input', 'log_iter': 10, 'max_epochs': 15, 'batch_size': 60, 'lr': 0.001, 'n_workers': 8}
2026-01-23 00:57:33,413 - Learnable parameters: 4416480
2026-01-23 00:57:42,030 - 
[Epoch 0 Training]
2026-01-23 00:57:44,818 - Iter 0/1891, Loss=0.4802, Loss=0.4802
2026-01-23 00:57:53,112 - Iter 10/1891, Loss=0.0736, Loss=0.0736
2026-01-23 00:58:01,465 - Iter 20/1891, Loss=0.0711, Loss=0.0711
2026-01-23 00:58:09,752 - Iter 30/1891, Loss=0.0693, Loss=0.0693
2026-01-23 00:58:18,057 - Iter 40/1891, Loss=0.0691, Loss=0.0691
2026-01-23 00:58:26,377 - Iter 50/1891, Loss=0.0702, Loss=0.0702
2026-01-23 00:58:34,751 - Iter 60/1891, Loss=0.0694, Loss=0.0694
2026-01-23 00:58:43,089 - Iter 70/1891, Loss=0.0706, Loss=0.0706
2026-01-23 00:58:51,385 - Iter 80/1891, Loss=0.0698, Loss=0.0698
2026-01-23 00:58:59,776 - Iter 90/1891, Loss=0.0708, Loss=0.0708
2026-01-23 00:59:08,165 - Iter 100/1891, Loss=0.0695, Loss=0.0695
2026-01-23 00:59:16,535 - Iter 110/1891, Loss=0.0704, Loss=0.0704
2026-01-23 00:59:24,884 - Iter 120/1891, Loss=0.0709, Loss=0.0709
2026-01-23 00:59:33,219 - Iter 130/1891, Loss=0.0689, Loss=0.0689
2026-01-23 00:59:41,562 - Iter 140/1891, Loss=0.0697, Loss=0.0697
