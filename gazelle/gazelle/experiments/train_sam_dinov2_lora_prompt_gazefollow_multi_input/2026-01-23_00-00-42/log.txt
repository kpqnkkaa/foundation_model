2026-01-23 00:00:42,252 - Experiment Config: {'model': 'sam_dinov2_vitb_lora', 'data_path': '/mnt/nvme1n1/lululemon/xjj/datasets/resized/gazefollow_extended', 'ckpt_save_dir': './experiments', 'wandb_project': 'sam_dinov2_vitb_lora_multi_input', 'exp_name': 'train_sam_dinov2_lora_prompt_gazefollow_multi_input', 'log_iter': 10, 'max_epochs': 15, 'batch_size': 60, 'lr': 0.001, 'n_workers': 8}
2026-01-23 00:02:56,379 - Learnable parameters: 4940640
2026-01-23 00:03:01,565 - 
[Epoch 0 Training]
2026-01-23 00:03:04,070 - Iter 0/1891, Loss=2.2998, Loss=2.2998
2026-01-23 00:03:12,309 - Iter 10/1891, Loss=1.2891, Loss=1.2891
2026-01-23 00:03:20,530 - Iter 20/1891, Loss=1.3320, Loss=1.3320
2026-01-23 00:03:28,795 - Iter 30/1891, Loss=1.3228, Loss=1.3228
2026-01-23 00:03:37,061 - Iter 40/1891, Loss=1.2819, Loss=1.2819
2026-01-23 00:03:45,330 - Iter 50/1891, Loss=1.3430, Loss=1.3430
2026-01-23 00:03:53,593 - Iter 60/1891, Loss=1.3349, Loss=1.3349
2026-01-23 00:04:01,906 - Iter 70/1891, Loss=1.3115, Loss=1.3115
2026-01-23 00:04:10,152 - Iter 80/1891, Loss=1.3145, Loss=1.3145
2026-01-23 00:04:18,426 - Iter 90/1891, Loss=1.2979, Loss=1.2979
2026-01-23 00:04:26,709 - Iter 100/1891, Loss=1.3362, Loss=1.3362
2026-01-23 00:04:34,981 - Iter 110/1891, Loss=1.3023, Loss=1.3023
2026-01-23 00:04:43,295 - Iter 120/1891, Loss=1.2889, Loss=1.2889
2026-01-23 00:04:51,573 - Iter 130/1891, Loss=1.3035, Loss=1.3035
2026-01-23 00:04:59,856 - Iter 140/1891, Loss=1.2898, Loss=1.2898
2026-01-23 00:05:08,121 - Iter 150/1891, Loss=1.2962, Loss=1.2962
2026-01-23 00:05:16,403 - Iter 160/1891, Loss=1.3429, Loss=1.3429
2026-01-23 00:05:24,698 - Iter 170/1891, Loss=1.3488, Loss=1.3488
2026-01-23 00:05:32,931 - Iter 180/1891, Loss=1.2939, Loss=1.2939
2026-01-23 00:05:41,203 - Iter 190/1891, Loss=1.3210, Loss=1.3210
2026-01-23 00:05:49,474 - Iter 200/1891, Loss=1.3464, Loss=1.3464
2026-01-23 00:05:57,755 - Iter 210/1891, Loss=1.3099, Loss=1.3099
2026-01-23 00:06:05,997 - Iter 220/1891, Loss=1.2942, Loss=1.2942
2026-01-23 00:06:14,243 - Iter 230/1891, Loss=1.2923, Loss=1.2923
2026-01-23 00:06:22,524 - Iter 240/1891, Loss=1.3209, Loss=1.3209
2026-01-23 00:06:30,794 - Iter 250/1891, Loss=1.3270, Loss=1.3270
2026-01-23 00:06:39,080 - Iter 260/1891, Loss=1.3161, Loss=1.3161
2026-01-23 00:06:47,360 - Iter 270/1891, Loss=1.3227, Loss=1.3227
2026-01-23 00:06:55,594 - Iter 280/1891, Loss=1.3089, Loss=1.3089
2026-01-23 00:07:03,835 - Iter 290/1891, Loss=1.2857, Loss=1.2857
2026-01-23 00:07:12,169 - Iter 300/1891, Loss=1.2699, Loss=1.2699
2026-01-23 00:07:20,448 - Iter 310/1891, Loss=1.2859, Loss=1.2859
2026-01-23 00:07:28,669 - Iter 320/1891, Loss=1.3448, Loss=1.3448
2026-01-23 00:07:36,924 - Iter 330/1891, Loss=1.3332, Loss=1.3332
2026-01-23 00:07:45,182 - Iter 340/1891, Loss=1.3212, Loss=1.3212
2026-01-23 00:07:53,418 - Iter 350/1891, Loss=1.3388, Loss=1.3388
2026-01-23 00:08:01,667 - Iter 360/1891, Loss=1.3024, Loss=1.3024
2026-01-23 00:08:09,941 - Iter 370/1891, Loss=1.3324, Loss=1.3324
2026-01-23 00:08:18,198 - Iter 380/1891, Loss=1.3141, Loss=1.3141
2026-01-23 00:08:26,463 - Iter 390/1891, Loss=1.3184, Loss=1.3184
2026-01-23 00:08:34,764 - Iter 400/1891, Loss=1.3022, Loss=1.3022
2026-01-23 00:08:43,058 - Iter 410/1891, Loss=1.3392, Loss=1.3392
