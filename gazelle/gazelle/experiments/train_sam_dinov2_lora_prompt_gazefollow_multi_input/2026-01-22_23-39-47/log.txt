2026-01-22 23:39:47,173 - Experiment Config: {'model': 'gazelle_dinov2_vitb14', 'data_path': '/mnt/nvme1n1/lululemon/xjj/datasets/resized/gazefollow_extended', 'ckpt_save_dir': './experiments', 'wandb_project': 'sam_dinov2_vitb_lora_multi_input', 'exp_name': 'train_sam_dinov2_lora_prompt_gazefollow_multi_input', 'log_iter': 10, 'max_epochs': 15, 'batch_size': 60, 'lr': 0.001, 'n_workers': 8}
2026-01-22 23:39:50,424 - Learnable parameters: 2826752
2026-01-22 23:39:56,233 - 
[Epoch 0 Training]
2026-01-22 23:39:57,916 - Iter 0/1891, Loss=0.7396, Loss=0.7396
2026-01-22 23:40:02,107 - Iter 10/1891, Loss=0.0714, Loss=0.0714
2026-01-22 23:40:06,308 - Iter 20/1891, Loss=0.0658, Loss=0.0658
2026-01-22 23:40:10,567 - Iter 30/1891, Loss=0.0617, Loss=0.0617
2026-01-22 23:40:14,864 - Iter 40/1891, Loss=0.0586, Loss=0.0586
2026-01-22 23:40:19,104 - Iter 50/1891, Loss=0.0574, Loss=0.0574
2026-01-22 23:40:23,390 - Iter 60/1891, Loss=0.0574, Loss=0.0574
2026-01-22 23:40:27,658 - Iter 70/1891, Loss=0.0610, Loss=0.0610
2026-01-22 23:40:31,910 - Iter 80/1891, Loss=0.0544, Loss=0.0544
2026-01-22 23:40:36,147 - Iter 90/1891, Loss=0.0557, Loss=0.0557
2026-01-22 23:40:40,396 - Iter 100/1891, Loss=0.0577, Loss=0.0577
2026-01-22 23:40:44,672 - Iter 110/1891, Loss=0.0580, Loss=0.0580
2026-01-22 23:40:48,935 - Iter 120/1891, Loss=0.0560, Loss=0.0560
2026-01-22 23:40:53,190 - Iter 130/1891, Loss=0.0547, Loss=0.0547
2026-01-22 23:40:57,444 - Iter 140/1891, Loss=0.0591, Loss=0.0591
2026-01-22 23:41:01,704 - Iter 150/1891, Loss=0.0596, Loss=0.0596
2026-01-22 23:41:05,989 - Iter 160/1891, Loss=0.0530, Loss=0.0530
2026-01-22 23:41:10,262 - Iter 170/1891, Loss=0.0573, Loss=0.0573
2026-01-22 23:41:14,569 - Iter 180/1891, Loss=0.0562, Loss=0.0562
2026-01-22 23:41:18,840 - Iter 190/1891, Loss=0.0533, Loss=0.0533
2026-01-22 23:41:23,135 - Iter 200/1891, Loss=0.0534, Loss=0.0534
