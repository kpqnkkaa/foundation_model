2026-01-23 09:55:36,406 - Experiment Config: {'model': 'sam_dinov2_vitb_lora', 'data_path': '/mnt/nvme1n1/lululemon/xjj/datasets/resized/gazefollow_extended', 'ckpt_save_dir': './experiments', 'wandb_project': 'sam_dinov2_vitb_lora_multi_input', 'exp_name': 'train_sam_dinov2_lora_prompt_gazefollow_multi_input', 'log_iter': 10, 'max_epochs': 15, 'batch_size': 60, 'lr': 0.001, 'n_workers': 8}
2026-01-23 09:55:40,773 - Learnable parameters: 4416480
2026-01-23 09:55:46,910 - 
[Epoch 0 Training]
2026-01-23 09:55:49,904 - Iter 0/1891, Loss=0.7125, Loss=0.7125
2026-01-23 09:55:59,967 - Iter 10/1891, Loss=0.1091, Loss=0.1091
2026-01-23 09:56:10,057 - Iter 20/1891, Loss=0.0704, Loss=0.0704
2026-01-23 09:56:20,119 - Iter 30/1891, Loss=0.0697, Loss=0.0697
2026-01-23 09:56:30,208 - Iter 40/1891, Loss=0.0692, Loss=0.0692
2026-01-23 09:56:40,335 - Iter 50/1891, Loss=0.0700, Loss=0.0700
2026-01-23 09:56:50,380 - Iter 60/1891, Loss=0.0692, Loss=0.0692
2026-01-23 09:57:00,407 - Iter 70/1891, Loss=0.0705, Loss=0.0705
2026-01-23 09:57:10,414 - Iter 80/1891, Loss=0.0690, Loss=0.0690
2026-01-23 09:57:20,412 - Iter 90/1891, Loss=0.0707, Loss=0.0707
2026-01-23 09:57:30,395 - Iter 100/1891, Loss=0.0688, Loss=0.0688
2026-01-23 09:57:40,409 - Iter 110/1891, Loss=0.0692, Loss=0.0692
2026-01-23 09:57:50,431 - Iter 120/1891, Loss=0.0699, Loss=0.0699
2026-01-23 09:58:00,450 - Iter 130/1891, Loss=0.0685, Loss=0.0685
2026-01-23 09:58:10,401 - Iter 140/1891, Loss=0.0694, Loss=0.0694
2026-01-23 09:58:20,393 - Iter 150/1891, Loss=0.0687, Loss=0.0687
2026-01-23 09:58:30,404 - Iter 160/1891, Loss=0.0689, Loss=0.0689
2026-01-23 09:58:40,337 - Iter 170/1891, Loss=0.0705, Loss=0.0705
2026-01-23 09:58:50,332 - Iter 180/1891, Loss=0.0697, Loss=0.0697
2026-01-23 09:59:00,247 - Iter 190/1891, Loss=0.0688, Loss=0.0688
2026-01-23 09:59:10,205 - Iter 200/1891, Loss=0.0688, Loss=0.0688
2026-01-23 09:59:20,354 - Iter 210/1891, Loss=0.0696, Loss=0.0696
