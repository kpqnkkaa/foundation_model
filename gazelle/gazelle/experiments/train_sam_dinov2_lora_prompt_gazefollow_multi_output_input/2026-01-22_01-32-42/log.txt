2026-01-22 01:32:42,321 - Experiment Config: {'model': 'sam_dinov2_vitb_lora_multi_output_input', 'data_path': '/mnt/nvme1n1/lululemon/xjj/datasets/resized/gazefollow_extended', 'ckpt_save_dir': './experiments', 'wandb_project': 'sam_dinov2_vitb_lora_multi_output_input', 'exp_name': 'train_sam_dinov2_lora_prompt_gazefollow_multi_output_input', 'log_iter': 10, 'max_epochs': 15, 'batch_size': 60, 'lr': 0.001, 'n_workers': 8}
2026-01-22 01:32:53,461 - Learnable parameters: 7463936
2026-01-22 01:33:05,463 - 
[Epoch 0 Training]
2026-01-22 01:33:08,943 - Iter 0/1891, Loss=0.7159, Loss=0.7159
2026-01-22 01:33:15,955 - Iter 10/1891, Loss=0.1135, Loss=0.1135
2026-01-22 01:33:22,099 - Iter 20/1891, Loss=0.0635, Loss=0.0635
2026-01-22 01:33:28,248 - Iter 30/1891, Loss=0.0611, Loss=0.0611
2026-01-22 01:33:34,312 - Iter 40/1891, Loss=0.0597, Loss=0.0597
2026-01-22 01:33:40,259 - Iter 50/1891, Loss=0.0557, Loss=0.0557
2026-01-22 01:33:46,813 - Iter 60/1891, Loss=0.0557, Loss=0.0557
2026-01-22 01:33:52,809 - Iter 70/1891, Loss=0.0569, Loss=0.0569
2026-01-22 01:33:58,782 - Iter 80/1891, Loss=0.0580, Loss=0.0580
2026-01-22 01:34:04,790 - Iter 90/1891, Loss=0.0530, Loss=0.0530
2026-01-22 01:34:10,750 - Iter 100/1891, Loss=0.0513, Loss=0.0513
2026-01-22 01:34:16,723 - Iter 110/1891, Loss=0.0529, Loss=0.0529
2026-01-22 01:34:22,779 - Iter 120/1891, Loss=0.0467, Loss=0.0467
